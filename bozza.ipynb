{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axHQYI3joBAD"
      },
      "source": [
        "# Earthquake Data Analysis\n",
        "\n",
        "### Description\n",
        "\n",
        "The catalog includes the magnitude, time of occurrence (s), and 3D coordinates (m) of earthquakes in about 20 years of recording in South California. Coordinates were converted from latitude, longitude, and depth of events in a seismic catalog. Magnitudes should be within the range $[0,8]$.\n",
        "\n",
        "* **Waiting time (t)**: time interval between an event and the next one in the sequence.\n",
        "* **Distance (r)**: Eucledian 3D distance between events. (each 3D set of coordinates refers to the hypocenter, i.e. the point triggering the slip in a fault that forms the earthquake)\n",
        "\n",
        "\n",
        "### Assignments\n",
        "\n",
        "1. Deduce what is the variable in each column of the catalog.\n",
        "2. Visualize the process in space and/or time with suitable time series and/or 3D visualizations of the hypocenters. For instance, plot a space variable (a single coordinate or a nice linear combination of coordinates) as a function of time.\n",
        "3. Compute the distribution $P_m(t)$ of waiting times for events of magnitude m or above (i.e. do not consider events below $m$). In shaping the bin sizes, take into account that this distribution is expected to have a power-law decay with time (e.g $\\sim 1/t$), and that a power-law is well visualized in log-log scale. Do this analysis for many values of $m$, say $m=2,3,4,5$.\n",
        "4. Compute the distribution $P_m(r)$ of the distance between an event and the next one, considering earthquakes of magnitude m or above. Also here make a clever choice for the bin sizes and try several values of $m$.\n",
        "5. Compute the distribution $P_{m,R}(t)$ of waiting times for events of magnitude $m$ or above, which are separated by at most a distance $r<R$, for different values of m and $R$. (In this statistics, if the following event is farther than $R$, skip the $t$ and go to the next pair)\n",
        "6. Eventually note if, from the analysis of the previous points, there emerges a scaling picture. Is there a suitable rescaling that collapses distributions for various $m$ (and eventually $R$ if point 5 is considered) on a single curve?\n",
        "\n",
        "### Datasets\n",
        "\n",
        "* column 1: index of the event\n",
        "* column 2: index of the previous event that triggered it (defined with a given algorithm), -1 if no ancestor is found\n",
        "* column 3: time (seconds) from 0:00 of Jan.1st, 1982\n",
        "* column 4: magnitude\n",
        "* columns 5, 6, and 7: 3D coordinates (meters) of the earthquake hypocenter, i.e. of the point from where it started. These Euclidean coordinates are derived from latitude, longitude and depth.\n",
        "\n",
        "Joining each event to that with the index of the second column (if not -1), there emerges a set of causal trees.\n",
        "\n",
        "\n",
        "### Contact\n",
        "* Marco Baiesi <marco.baiesi@unipd.it>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkBNojy6idDx"
      },
      "source": [
        "# 1. Data Acquisition\n",
        "\n",
        "We load the given dataset, organize it in a pandas DataFrame and assign a name to each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5pY4JPooJhW",
        "outputId": "e84587f5-951e-4571-ade0-b03d4b9aadd8"
      },
      "outputs": [],
      "source": [
        "# google colab upload link, to be omitted when working on Jupyter Notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "filepath = '/content/drive/MyDrive/LCP_project/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRPDF8RYZN6r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import plotly.express as px\n",
        "import matplotlib as mpl\n",
        "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
        "import seaborn as sns\n",
        "import numpy.linalg as la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9T7DHoSoBAM"
      },
      "outputs": [],
      "source": [
        "data = 'SouthCalifornia-1982-2011_Physics-of-Data.dat'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "jynXRx7UoBAQ",
        "outputId": "3e92c4e1-7d07-447b-9ab6-88b39a6ab826"
      },
      "outputs": [],
      "source": [
        "# google colab command\n",
        "df = pd.read_csv(data, sep = ' ', names = ['Index', 'Prev_event', 'Time', 'Magnitude', 'x', 'y', 'z'])\n",
        "\n",
        "# Jupyter Notebook command\n",
        "# df = pd.read_csv(data, sep = ' ', names = ['Index', 'Prev_event', 'Time', 'Magnitude', 'x', 'y', 'z'])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "qb0RkEJnZN6s",
        "outputId": "f4fd7799-f16e-41d1-db4f-38e7a2711dc9"
      },
      "outputs": [],
      "source": [
        "# AGGIUNTA COLONNE PER I DATI AGGIUNTIVI: Waiting time, Distance, lat, lon, dep\n",
        "df['Waiting time prev'] = df['Index']\n",
        "df['Distances prev'] = df['Index']\n",
        "df['Waiting time'] = df['Index']\n",
        "df['Distances'] = df['Index']\n",
        "\n",
        "for i in range(np.shape(df)[0]):\n",
        "  if df.loc[i, 'Prev_event'] == -1:\n",
        "    Time_1 = df.loc[i, 'Time']\n",
        "    Distance_1 = np.array([df['x'][i], df['y'][i], df['z'][i]])\n",
        "  elif df.loc[i, 'Prev_event'] > -1:\n",
        "    Time_1 = df['Time'][df['Prev_event'][i]]\n",
        "    Distance_1 = np.array([df['x'][df['Prev_event'][i]], df['y'][df['Prev_event'][i]], df['z'][df['Prev_event'][i]]])\n",
        "\n",
        "  df.loc[i, 'Waiting time prev'] = df['Time'][i] - Time_1\n",
        "  df.loc[i, 'Distances prev'] = math.sqrt(np.sum((np.array([df['x'][i], df['y'][i], df['z'][i]]) - Distance_1)**2))\n",
        "\n",
        "\n",
        "for i in range(np.shape(df)[0]):\n",
        "  if df['Index'][i] == 0:\n",
        "    Time_1 = df.loc[i, 'Time']\n",
        "    Distance_1 = np.array([df.loc[i, 'x'], df.loc[i, 'y'], df.loc[i, 'z']])\n",
        "  elif df.loc[i, 'Index'] > -0:\n",
        "    Time_1 = df.loc[i-1, 'Time']\n",
        "    Distance_1 = np.array([df.loc[i-1, 'x'], df.loc[i-1, 'y'], df.loc[i-1, 'z']])\n",
        "\n",
        "  df.loc[i, 'Waiting time'] = df.loc[i, 'Time'] - Time_1\n",
        "  df.loc[i, 'Distances'] = math.sqrt(np.sum((np.array([df.loc[i, 'x'], df.loc[i, 'y'], df.loc[i, 'z']]) - Distance_1)**2))\n",
        "\n",
        "R = 6371000 # m\n",
        "df['r'] = df['Index']\n",
        "for i in range(np.shape(df)[0]):\n",
        "  df.loc[i, 'r'] = (df.loc[i, 'x']**2 + df.loc[i, 'y']**2 + df.loc[i, 'z']**2)**0.5\n",
        "\n",
        "df['lat'] = np.arcsin(df['z'] / df['r'])*180/math.pi\n",
        "df['lon'] = np.arctan2(df['y'], df['x'])*180/math.pi\n",
        "df['dep'] = df['r'] - R\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBimq9g_ZN6l"
      },
      "source": [
        "# 2. Graphical Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3gpoIjAmQWj"
      },
      "source": [
        "## 2.1 Plots of the geographical coordinates\n",
        "\n",
        "In the followig section we present some 2D and 3D plots of the earthquakes' hypocenter locations with respect to the time and magnitude variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMRBOsl25lqv"
      },
      "source": [
        "### 2.1.1 Static maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "lPRKxfKjZN6u",
        "outputId": "52fc9853-743f-469b-8ad0-e7d3d690e5dd"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(16,9))\n",
        "\n",
        "ax1 = fig.add_subplot(2, 3, 1, projection='3d')\n",
        "scatter1 = ax1.scatter(df['x'], df['y'], df['z'], c=df['dep'], cmap='inferno_r', marker='.')\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.set_zlabel('z')\n",
        "ax1.view_init(0,30)\n",
        "fig.colorbar(scatter1, label='depth', shrink=0.5)\n",
        "\n",
        "ax2 = fig.add_subplot(2, 3, 2)\n",
        "scatter2 = ax2.scatter(df['x'], df['y'], c=df['dep'], cmap='inferno_r', marker='.')\n",
        "ax2.set_xlabel('x')\n",
        "ax2.set_ylabel('y')\n",
        "ax2.set_aspect(aspect=1)\n",
        "fig.colorbar(scatter2, label='depth', shrink=0.7)\n",
        "\n",
        "ax3 = fig.add_subplot(2, 3, 3, projection='3d')\n",
        "scatter3 = ax3.scatter(df['x'], df['y'], df['z'], c=df['dep'], cmap='inferno_r', marker='.')\n",
        "ax3.set_xlabel('x')\n",
        "ax3.set_ylabel('y')\n",
        "ax3.set_zlabel('z')\n",
        "ax3.view_init(0,150)\n",
        "fig.colorbar(scatter3, label='depth', shrink=0.5)\n",
        "\n",
        "ax4 = fig.add_subplot(2, 3, 4, projection='3d')\n",
        "scatter4 = ax4.scatter(df['lon'], df['lat'], df['dep'], c=df['dep'], cmap='inferno_r', marker='.')\n",
        "ax4.set_xlabel('longitude')\n",
        "ax4.set_ylabel('latitude')\n",
        "ax4.set_zlabel('depth')\n",
        "ax4.view_init(20,90)\n",
        "fig.colorbar(scatter4, label='depth', shrink=0.5)\n",
        "\n",
        "ax5 = fig.add_subplot(2, 3, 5)\n",
        "scatter5 = ax5.scatter(df['lon'], df['lat'], c=df['dep'], cmap='inferno_r', marker='.')\n",
        "ax5.set_xlabel('longitude')\n",
        "ax5.set_ylabel('latitude')\n",
        "ax5.set_aspect(aspect=1)\n",
        "fig.colorbar(scatter5, label='depth', shrink=0.7)\n",
        "\n",
        "df_mag4 = df[df['Magnitude'] > 4]\n",
        "ax6 = fig.add_subplot(2, 3, 6, projection='3d')\n",
        "scatter6 = ax6.scatter(df_mag4['lon'], df_mag4['lat'], df_mag4['dep'], c=df_mag4['dep'], cmap='inferno_r', marker='.')\n",
        "ax6.set_xlabel('longitude')\n",
        "ax6.set_ylabel('latitude')\n",
        "ax6.set_zlabel('depth')\n",
        "ax6.set_title('Magnitude > 4')\n",
        "ax6.view_init(20,90)\n",
        "fig.colorbar(scatter6, label='depth', shrink=0.5)\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "id": "vWpTxR1wZN6w",
        "outputId": "97010c9e-7c01-4161-fb3c-bfe2acfcbebd"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(16,9))\n",
        "\n",
        "ax1 = fig.add_subplot(2, 3, 1, projection='3d')\n",
        "scatter1 = ax1.scatter(df['x'], df['y'], df['z'], c=np.log(df['Waiting time']), cmap='viridis', marker='.')\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.set_zlabel('z')\n",
        "ax1.view_init(0,30)\n",
        "fig.colorbar(scatter1, label='Waiting time log', shrink=0.5)\n",
        "\n",
        "ax2 = fig.add_subplot(2, 3, 2)\n",
        "scatter2 = ax2.scatter(df['x'], df['y'], c=np.log(df['Waiting time']), cmap='viridis', marker='.')\n",
        "ax2.set_xlabel('x')\n",
        "ax2.set_ylabel('y')\n",
        "ax2.set_aspect(aspect=1)\n",
        "fig.colorbar(scatter2, label='Waiting time log', shrink=0.7)\n",
        "\n",
        "ax3 = fig.add_subplot(2, 3, 3, projection='3d')\n",
        "scatter3 = ax3.scatter(df['x'], df['y'], df['z'], c=df['Time'], cmap='viridis', marker='.')\n",
        "ax3.set_xlabel('x')\n",
        "ax3.set_ylabel('y')\n",
        "ax3.set_zlabel('z')\n",
        "ax3.view_init(0,30)\n",
        "fig.colorbar(scatter3, label='Time', shrink=0.5)\n",
        "\n",
        "ax4 = fig.add_subplot(2, 3, 4, projection='3d')\n",
        "scatter4 = ax4.scatter(df['lon'], df['lat'], df['dep'], c=np.log(df['Waiting time']), cmap='viridis', marker='.')\n",
        "ax4.set_xlabel('longtude')\n",
        "ax4.set_ylabel('latitude')\n",
        "ax4.set_zlabel('depth')\n",
        "ax4.view_init(20,90)\n",
        "fig.colorbar(scatter4, label='Waiting time log', shrink=0.5)\n",
        "\n",
        "ax5 = fig.add_subplot(2, 3, 5)\n",
        "scatter5 = ax5.scatter(df['lon'], df['lat'], c=np.log(df['Waiting time']), cmap='viridis', marker='.')\n",
        "ax5.set_xlabel('longitude')\n",
        "ax5.set_ylabel('latitude')\n",
        "ax5.set_aspect(aspect=1)\n",
        "fig.colorbar(scatter5, label='Waiting time log', shrink=0.7)\n",
        "\n",
        "ax6 = fig.add_subplot(2, 3, 6, projection='3d')\n",
        "scatter6 = ax6.scatter(df['lon'], df['lat'], df['dep'], c=df['Time'], cmap='viridis', marker='.')\n",
        "ax6.set_xlabel('longitude')\n",
        "ax6.set_ylabel('latitude')\n",
        "ax6.set_zlabel('depth')\n",
        "ax6.view_init(20,90)\n",
        "fig.colorbar(scatter6, label='Time', shrink=0.5)\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "bqt7QbUBZN6x",
        "outputId": "21ebd6b7-8744-4f21-e50a-75a53c026d79"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(16,9))\n",
        "\n",
        "ax1 = fig.add_subplot(2, 3, 1, projection='3d')\n",
        "scatter1 = ax1.scatter(df['x'], df['y'], df['z'], c=df['Magnitude'], cmap='plasma', marker='.', vmin=np.min(df.Magnitude), vmax=np.max(df.Magnitude))\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.set_zlabel('z')\n",
        "ax1.view_init(0,30)\n",
        "fig.colorbar(scatter1, label='Magnitude', shrink=0.5)\n",
        "\n",
        "ax2 = fig.add_subplot(2, 3, 2)\n",
        "scatter2 = ax2.scatter(df['x'], df['y'], c=np.log(df['Magnitude']), cmap='plasma', marker='.', vmin=np.min(np.log(df.Magnitude)), vmax=np.max(np.log(df.Magnitude)))\n",
        "ax2.set_xlabel('x')\n",
        "ax2.set_ylabel('y')\n",
        "ax2.set_aspect(aspect=1)\n",
        "fig.colorbar(scatter2, label='Magnitude log', shrink=0.7)\n",
        "\n",
        "df_mag3 = df[df['Magnitude'] > 3]\n",
        "ax3 = fig.add_subplot(2, 3, 3, projection='3d')\n",
        "scatter3 = ax3.scatter(df_mag3['x'], df_mag3['y'], df_mag3['z'], c=np.log(df_mag3['Magnitude']), cmap='plasma', marker='.', vmin=np.min(np.log(df.Magnitude)), vmax=np.max(np.log(df.Magnitude)))\n",
        "ax3.set_xlabel('x')\n",
        "ax3.set_ylabel('y')\n",
        "ax3.set_zlabel('z')\n",
        "ax3.view_init(0,30)\n",
        "fig.colorbar(scatter3, label='Magnitude log > 3', shrink=0.5)\n",
        "\n",
        "df_mag4 = df[df['Magnitude'] > 4]\n",
        "ax4 = fig.add_subplot(2, 3, 4, projection='3d')\n",
        "scatter4 = ax4.scatter(df_mag4['x'], df_mag4['y'], df_mag4['z'], c=df_mag4['Magnitude'], cmap='plasma', marker='.', vmin=np.min(df.Magnitude), vmax=np.max(df.Magnitude))\n",
        "ax4.set_xlabel('x')\n",
        "ax4.set_ylabel('y')\n",
        "ax4.set_zlabel('z')\n",
        "ax4.view_init(0,30)\n",
        "fig.colorbar(scatter4, label='Magnitude > 4', shrink=0.5)\n",
        "\n",
        "ax5 = fig.add_subplot(2, 3, 5)\n",
        "scatter5 = ax5.scatter(df_mag4['x'], df_mag4['y'], c=np.log(df_mag4['Magnitude']), cmap='plasma', marker='.', vmin=np.min(np.log(df.Magnitude)), vmax=np.max(np.log(df.Magnitude)))\n",
        "ax5.set_xlabel('x')\n",
        "ax5.set_ylabel('y')\n",
        "ax5.set_aspect(aspect=1)\n",
        "fig.colorbar(scatter5, label='Magnitude log > 4', shrink=0.7)\n",
        "\n",
        "df_mag6 = df[df['Magnitude'] > 6]\n",
        "ax6 = fig.add_subplot(2, 3, 6, projection='3d')\n",
        "scatter6 = ax6.scatter(df_mag6['x'], df_mag6['y'], df_mag6['z'], c=df_mag6['Magnitude'], cmap='plasma', marker='o', alpha=0.8, vmin=np.min(df.Magnitude), vmax=np.max(df.Magnitude))\n",
        "ax6.set_xlabel('x')\n",
        "ax6.set_ylabel('y')\n",
        "ax6.set_zlabel('z')\n",
        "ax6.view_init(0,30)\n",
        "fig.colorbar(scatter6, label='Magnitude log > 6', shrink=0.5)\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "Vhf5IJjFZN6y",
        "outputId": "f5af0fc6-bfc5-498d-e785-8efa4e5d3b25"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(16,10))\n",
        "\n",
        "ax1 = fig.add_subplot(2, 3, 1, projection='3d')\n",
        "scatter1 = ax1.scatter(df['lon'], df['lat'], df['dep'], c=df['Magnitude'], cmap='plasma', marker='.', vmin=np.min(df.Magnitude), vmax=np.max(df.Magnitude))\n",
        "ax1.set_xlabel('longitude')\n",
        "ax1.set_ylabel('latitude')\n",
        "ax1.set_zlabel('depth')\n",
        "ax1.view_init(20,90)\n",
        "fig.colorbar(scatter1, label='Magnitude', shrink=0.5)\n",
        "\n",
        "ax2 = fig.add_subplot(2, 3, 2)\n",
        "scatter2 = ax2.scatter(df['lon'], df['lat'], c=np.log(df['Magnitude']), cmap='plasma', marker='.', vmin=np.min(np.log(df.Magnitude)), vmax=np.max(np.log(df.Magnitude)))\n",
        "ax2.set_xlabel('longitude')\n",
        "ax2.set_ylabel('latitude')\n",
        "ax2.set_aspect(aspect=1)\n",
        "fig.colorbar(scatter2, label='Magnitude log', shrink=0.7)\n",
        "\n",
        "df_mag3 = df[df['Magnitude'] > 3]\n",
        "ax3 = fig.add_subplot(2, 3, 3, projection='3d')\n",
        "scatter3 = ax3.scatter(df_mag3['lon'], df_mag3['lat'], df_mag3['dep'], c=np.log(df_mag3['Magnitude']), cmap='plasma', marker='.', vmin=np.min(np.log(df.Magnitude)), vmax=np.max(np.log(df.Magnitude)))\n",
        "ax3.set_xlabel('longitude')\n",
        "ax3.set_ylabel('latitude')\n",
        "ax3.set_zlabel('depth')\n",
        "ax3.view_init(20,90)\n",
        "fig.colorbar(scatter3, label='Magnitude log > 3', shrink=0.5)\n",
        "\n",
        "df_mag4 = df[df['Magnitude'] > 4]\n",
        "ax4 = fig.add_subplot(2, 3, 4, projection='3d')\n",
        "scatter4 = ax4.scatter(df_mag4['lat'], df_mag4['lon'], df_mag4['dep'], c=df_mag4['Magnitude'], cmap='plasma', marker='.', vmin=np.min(df.Magnitude), vmax=np.max(df.Magnitude))\n",
        "ax4.set_xlabel('longitude')\n",
        "ax4.set_ylabel('latitude')\n",
        "ax4.set_zlabel('depth')\n",
        "ax4.view_init(20,90)\n",
        "fig.colorbar(scatter4, label='Magnitude > 4', shrink=0.5)\n",
        "\n",
        "ax5 = fig.add_subplot(2, 3, 5)\n",
        "scatter5 = ax5.scatter(df_mag4['lon'], df_mag4['lat'], c=np.log(df_mag4['Magnitude']), cmap='plasma', marker='.', vmin=np.min(np.log(df.Magnitude)), vmax=np.max(np.log(df.Magnitude)))\n",
        "ax5.set_xlabel('longitude')\n",
        "ax5.set_ylabel('latitude')\n",
        "ax5.set_aspect(aspect=1)\n",
        "fig.colorbar(scatter5, label='Magnitude log > 4', shrink=0.7)\n",
        "\n",
        "df_mag6 = df[df['Magnitude'] > 6]\n",
        "ax6 = fig.add_subplot(2, 3, 6, projection='3d')\n",
        "scatter6 = ax6.scatter(df_mag6['lon'], df_mag6['lat'], df_mag6['dep'], c=df_mag6['Magnitude'], cmap='plasma', marker='o', alpha=0.8, vmin=np.min(df.Magnitude), vmax=np.max(df.Magnitude))\n",
        "ax6.set_xlabel('longitude')\n",
        "ax6.set_ylabel('latitude')\n",
        "ax6.set_zlabel('depth')\n",
        "ax6.view_init(20,90)\n",
        "fig.colorbar(scatter6, label='Magnitude log > 6', shrink=0.5)\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwEy7CiD5PKS"
      },
      "source": [
        "### 2.1.2 Tree structure visualization (a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "RK4JqXAIZN6y",
        "outputId": "2f77e148-9228-4025-e278-f84f07066462"
      },
      "outputs": [],
      "source": [
        "df_0 = df[df['Prev_event'] > 0]\n",
        "df_1 = df[df['Prev_event'] == 0]\n",
        "print(np.shape(df_0))\n",
        "\n",
        "fig = plt.figure(figsize=(16,5))\n",
        "\n",
        "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
        "scatter1_1 = ax1.scatter(df_0['x'], df_0['y'], df_0['z'], c=df_0['Prev_event'], cmap='rainbow', marker='.')\n",
        "scatter1_2 = ax1.scatter(df_1['x'], df_1['y'], df_1['z'], c='black', marker='.')\n",
        "fig.colorbar(scatter1_1, label='Prev_event', shrink=0.5)\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.set_zlabel('z')\n",
        "ax1.view_init(0,30)\n",
        "\n",
        "ax2= fig.add_subplot(1, 2, 2, projection='3d')\n",
        "scatter2_1 = ax2.scatter(df_0['lon'], df_0['lat'], df_0['dep'], c=df_0['Prev_event'], cmap='rainbow', marker='.')\n",
        "scatter2_2 = ax2.scatter(df_1['lon'], df_1['lat'], df_1['dep'], c='black', marker='.')\n",
        "fig.colorbar(scatter2_1, label='Prev_event', shrink=0.5)\n",
        "ax2.set_xlabel('longitude')\n",
        "ax2.set_ylabel('latitude')\n",
        "ax2.set_zlabel('depth')\n",
        "ax2.view_init(20,90)\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4_gJXFB5Xow"
      },
      "source": [
        "### 2.1.3 Interactive and animated maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TKi_v4ZLZN6z",
        "outputId": "850e8c18-2ee7-4cc4-dea5-95b6a2621ab4"
      },
      "outputs": [],
      "source": [
        "ax1 = px.scatter_mapbox(df, lat=df.lat, lon=df.lon, color=df.Magnitude, width=600, height=500, opacity=0.5, color_continuous_scale='plasma',\n",
        "                        center=dict(lat=32, lon=-117), zoom=4, title='Interactive map of the South California earthquakes',\n",
        "                        mapbox_style='open-street-map')\n",
        "ax1.show()\n",
        "\n",
        "ax2 = px.scatter_mapbox(df, lat=df.lat, lon=df.lon, color=df.dep, width=600, height=500, opacity=0.5, color_continuous_scale='inferno_r',\n",
        "                        center=dict(lat=32, lon=-117), zoom=4, title='Interactive map of the South California earthquakes',\n",
        "                        mapbox_style='open-street-map')\n",
        "ax2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wl2d28tDZN6z"
      },
      "outputs": [],
      "source": [
        "df['Animation_frame'] = np.round(200*(df['Time']/df['Time'].max()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HQHtOlDlZN60",
        "outputId": "eacadd4b-4be9-4da4-a978-5bac4d7ac235"
      },
      "outputs": [],
      "source": [
        "ax1 = px.scatter_mapbox(df, lat=df.lat, lon=df.lon, color=df.Magnitude, width=600, height=500, opacity=1, color_continuous_scale='plasma', range_color=[df.Magnitude.min(), df.Magnitude.max()],\n",
        "                        center=dict(lat=33.5, lon=-117), zoom=4, title='Interactive map of the South California earthquakes', animation_frame=df.Animation_frame,\n",
        "                        mapbox_style='open-street-map')\n",
        "ax1.show()\n",
        "\n",
        "ax2 = px.scatter_mapbox(df, lat=df.lat, lon=df.lon, color=df.dep, width=600, height=500, opacity=1, color_continuous_scale='inferno_r', range_color=[df.dep.min(), df.dep.max()],\n",
        "                        center=dict(lat=33.5, lon=-117), zoom=4, title='Interactive map of the South California earthquakes', animation_frame=df.Animation_frame,\n",
        "                        mapbox_style='open-street-map')\n",
        "ax2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXBMaOrY5-OF"
      },
      "source": [
        "### 2.1.4 Distribution plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "m9oc40aIZN60",
        "outputId": "40b540ed-fa06-42f9-bdeb-6302a2ca3cc8"
      },
      "outputs": [],
      "source": [
        "sns.jointplot(df, x=df.lon, y=df.lat, kind='hist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "--hFKUcpZN61",
        "outputId": "a28d639b-934b-4cb9-f86e-fdf8407cae14"
      },
      "outputs": [],
      "source": [
        "sns.kdeplot(df, x=df.lon, y=df.lat, fill=True, thresh=0, levels=15, cmap='rocket_r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "E4AwV3Y6ZN61",
        "outputId": "52571d46-8dec-4ee2-ea67-c2df2ea6b247"
      },
      "outputs": [],
      "source": [
        "sns.jointplot(df, x=df.dep, y=df.Magnitude, kind='hist')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC3U6M8q6FVx"
      },
      "source": [
        "### 2.1.5 Time analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "9Kb_MEc7ZN61",
        "outputId": "89425085-8487-404a-fe92-3b050d01971c"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(15,4))\n",
        "\n",
        "ax1 = fig.add_subplot(1,3,1)\n",
        "ax1.plot(df.Time, df.Index, label='Time vs Index')\n",
        "ax1.set_xlabel('Time')\n",
        "ax1.set_ylabel('Index')\n",
        "\n",
        "i=100000\n",
        "ax2 = fig.add_subplot(1,3,2)\n",
        "ax2.scatter(df.Time[i:i+1000], df.Magnitude[i:i+1000], marker='.')\n",
        "ax2.set_xlabel('Time')\n",
        "ax2.set_ylabel('Magnitude')\n",
        "\n",
        "ax3 = fig.add_subplot(1,3,3)\n",
        "ax3.scatter(df.Magnitude[i:i+1000], df.dep[i:i+1000], marker='.')\n",
        "ax3.set_xlabel('Magnitude')\n",
        "ax3.set_ylabel('Depth')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "u8ZTy9pD0EVi",
        "outputId": "23fa7d1f-3fd1-4182-cb60-f062912da4c4"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(13,3.5))\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "\n",
        "n, bins, _ = ax.hist(df.Time, bins=5*int(np.sqrt(np.shape(df)[0])))\n",
        "\n",
        "ax.set_xticks([i for i in 0.5e8*np.arange(20)])\n",
        "ax.set_xlim(0, np.max(df.Time))\n",
        "ax.set_xlabel('Time (10^8 s)')\n",
        "ax.set_ylabel('Counts')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "Q2Ho-lDN-gm2",
        "outputId": "9d2869a1-886f-4726-b71f-6dfc3b2ea318"
      },
      "outputs": [],
      "source": [
        "centroids = bins[:-1] - bins[1:]\n",
        "event_time_dist = np.concatenate((centroids.reshape(np.shape(centroids)[0], 1), n.reshape(np.shape(n)[0], 1)), axis=1)\n",
        "\n",
        "from scipy.fftpack import fft, ifft\n",
        "spectrum = abs(fft(n))\n",
        "\n",
        "fig = plt.figure(figsize=(13,3.5))\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.scatter(np.arange(1660), spectrum, marker='.')\n",
        "\n",
        "np.shape(spectrum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "5HDgaoFtRKLX",
        "outputId": "453cd938-c663-4f17-c9d1-8975d648308f"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(13,3.5))\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "\n",
        "n, bins, _ = ax.hist(df['Waiting time'], bins=5*int(np.sqrt(np.shape(df)[0])))\n",
        "\n",
        "ax.set_xticks([i for i in 10000*np.arange(11)])\n",
        "ax.set_xlim(0, 1e5)\n",
        "ax.set_xlabel('Waiting time (s)')\n",
        "ax.set_ylabel('Counts')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYIjKJZhZN62"
      },
      "source": [
        "## 2.2 Principal Component Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "j54WV_32ZN62",
        "outputId": "83ce7dd1-5cce-42e2-bc3b-d5c1487a6f23"
      },
      "outputs": [],
      "source": [
        "PCA_array = np.array(df)[:, 4:7]\n",
        "print(np.shape(PCA_array))\n",
        "\n",
        "C = np.cov(PCA_array.T)\n",
        "sns.heatmap(C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vxzJ4AvZN63"
      },
      "outputs": [],
      "source": [
        "U, spectrum, Vt = la.svd(PCA_array[:10000].T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqKRkVrcZN63",
        "outputId": "0897bb76-613f-4021-df3d-d208142b69ae"
      },
      "outputs": [],
      "source": [
        "l_svd = spectrum**2/(np.shape(df)[0]+1)\n",
        "V_svd = U\n",
        "\n",
        "print('Autovalori\\t', l_svd/C.trace())\n",
        "print('Autovettore\\t', V_svd[:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fbW7E0xZN63"
      },
      "outputs": [],
      "source": [
        "array_newbasis = np.array([la.solve(U, PCA_array[i, :]) for i in range(np.shape(PCA_array)[0])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-V9_dXVZN64",
        "outputId": "a9197f59-52b2-409a-8f19-ed0d9b0326ee"
      },
      "outputs": [],
      "source": [
        "np.shape(array_newbasis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7iW08y6ZN64"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(16,5))\n",
        "\n",
        "ax1 = fig.add_subplot(1,2,1, projection='3d')\n",
        "\n",
        "scat1 = ax1.scatter(array_newbasis[:,0], array_newbasis[:,1], array_newbasis[:,2], marker='.', c=array_newbasis[:,0], cmap='inferno')\n",
        "ax1.set_xlabel('PC1')\n",
        "ax1.set_ylabel('PC2')\n",
        "ax1.set_zlabel('PC3')\n",
        "ax1.set_xlim(np.mean(array_newbasis[:,0])-1000000/2, np.mean(array_newbasis[:,0])+1000000/2)\n",
        "ax1.set_aspect(aspect='equal')\n",
        "fig.colorbar(scat1, label='PC1', shrink=0.5)\n",
        "\n",
        "ax2 = fig.add_subplot(1,2,2, projection='3d')\n",
        "\n",
        "scat2 = ax2.scatter(array_newbasis[:,0], array_newbasis[:,1], array_newbasis[:,2], marker='.', c=array_newbasis[:,0], cmap='inferno')\n",
        "ax2.set_xlabel('PC1')\n",
        "ax2.set_ylabel('PC2')\n",
        "ax2.set_zlabel('PC3')\n",
        "ax2.set_xlim(np.mean(array_newbasis[:,0])-1000000/2, np.mean(array_newbasis[:,0])+1000000/2)\n",
        "ax2.set_aspect(aspect='equal')\n",
        "ax2.view_init(0,90)\n",
        "fig.colorbar(scat2, label='PC1', shrink=0.5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po2J9Y3HZN68"
      },
      "source": [
        "# 3. Distribution $P_m(t)$ of the Waiting Times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMfYvuxRZN68"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from astropy.stats import knuth_bin_width as knuth\n",
        "from scipy.stats import chi2\n",
        "from scipy.optimize import curve_fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4Zzc4LTZN68"
      },
      "source": [
        "First we define the functions we need for data analysis and plots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncpIZKYlZN68"
      },
      "source": [
        "The `log_plot` function sets up all parameters we need for plotting histogram in log-log scale:\n",
        "\n",
        "* To select number of bins, we use used the Knuth binning rule (*$``\\text{Optimal Data-Based Binning for Histograms}\"$, Kevin H. Knuth, 2013*).\n",
        "\n",
        "* We generate an equally logarithmically spaced set of points using `np.logspace()` specifying the number of bins we calculated before.\n",
        "\n",
        "* After evaluating bin centers and bin widths we normalize the histogram in order to give a statistical meaning to the distribution.\n",
        "\n",
        "* We get rid of the empty bins in order for us to be able to evaluate the errors propagating the Poisson error of the unnormalized histogram.\n",
        "\n",
        "* The function returns the bin centers, the counts per bin and the errors (all in logarithmic scale)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoYsoZOQZN68"
      },
      "outputs": [],
      "source": [
        "def log_plot(feature):\n",
        "\n",
        "    #Excluding zeros\n",
        "    feature = feature[feature != 0]\n",
        "\n",
        "    #Discard consecutive elements of the feature to avoid correlations\n",
        "    feature = feature[::2]\n",
        "\n",
        "    #Bin edges (in logarithmic scale)\n",
        "    #n_bins = max([np.histogram_bin_edges(np.log10(feature), bins = 'scott').shape[0] - 1, 10])\n",
        "    _, bin_edges = knuth(np.log10(feature), return_bins = True)\n",
        "\n",
        "    x = np.logspace(np.log10(feature.min()), np.log10(feature.max()), base = 10, num = bin_edges.shape[0] - 1)\n",
        "\n",
        "    #Histogram data\n",
        "    hist = np.histogram(feature, bins = x)\n",
        "    bin_centers = (hist[1][1:] + hist[1][:-1]) / 2\n",
        "    bin_widths = hist[1][1:] - hist[1][:-1]\n",
        "\n",
        "    #Normalization of the histogram\n",
        "    norm = np.sum(hist[0])\n",
        "    counts_normalized = hist[0] / (bin_widths * norm)\n",
        "\n",
        "    #Clean of empty bins\n",
        "    mask = (counts_normalized > 0)\n",
        "    counts_normalized = counts_normalized[mask]\n",
        "    bin_centers = bin_centers[mask]\n",
        "\n",
        "    bins = np.log10(bin_centers)\n",
        "    counts = np.log10(counts_normalized)\n",
        "    errors = np.log10(np.e) / np.sqrt(hist[0][mask])\n",
        "\n",
        "    return bins, counts, errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkHLA14tZN69"
      },
      "source": [
        "As discussed later, we use `cut_tails` function to proper select the linearly distributed points (in log-log scale):\n",
        "\n",
        "* For the left side we first arbitraly drop off all data points a given threshold\n",
        "\n",
        "* Then, for both sides we recursively compare the pearson coefficient with the one we would obtain getting rid off of the extreme point: if the new value is better than the previous one we reject that point, otherwise we stop the algorithm.\n",
        "\n",
        "* The function returns bin centers, counts per bin, errors and the pearson coefficients list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_JGAb_xZN69"
      },
      "outputs": [],
      "source": [
        "def cut_tails(b, c, e, threshold):\n",
        "\n",
        "    c = c[b >= threshold]\n",
        "    e = e[b >= threshold]\n",
        "    b = b[b >= threshold]\n",
        "\n",
        "    pearson = [0]\n",
        "\n",
        "    while True:\n",
        "        new_p = np.corrcoef(b[:-1], y = c[:-1])[0,1]\n",
        "        if(new_p > pearson[-1] or len(b) <= 9):\n",
        "            break\n",
        "        b = b[:-1]\n",
        "        c = c[:-1]\n",
        "        e = e[:-1]\n",
        "        pearson.append(new_p)\n",
        "\n",
        "    while True:\n",
        "        new_p = np.corrcoef(b[1:], y = c[1:])[0,1]\n",
        "        if(new_p > pearson[-1] or len(b) <= 9):\n",
        "            break\n",
        "        b = b[1:]\n",
        "        c = c[1:]\n",
        "        e = e[1:]\n",
        "        pearson.append(new_p)\n",
        "\n",
        "    return b, c, e, pearson"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dENbURODZN6-"
      },
      "source": [
        "The `statistic_plots` function allows us to perform a linear regression on the given feature and to evaluate the $\\chi^2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "251LJRTuZN6-"
      },
      "outputs": [],
      "source": [
        "f_lin = lambda x, m, q: m*x+q\n",
        "\n",
        "def statistic_plots(bins, counts, magnitudes, errors, feature, spec = '', show_plots=True):\n",
        "    fit_res = []\n",
        "\n",
        "    if(show_plots):\n",
        "        fig, ax = plt.subplots(2, len(magnitudes), figsize = (14, 6))\n",
        "\n",
        "    for i, (x, y, m, e) in enumerate(zip(bins, counts, magnitudes, errors)):\n",
        "\n",
        "        #Linear regression\n",
        "        (slope, intercept), pcov = curve_fit(f_lin, x, y, sigma=e, absolute_sigma=True)\n",
        "        \n",
        "        #Residuals\n",
        "        residuals = y - (slope*x+intercept)\n",
        "        chisquare = np.sum((residuals/e)**2)\n",
        "        dof = x.shape[0]-2 #degrees of freedom of the chisquare\n",
        "        fit_res.append((slope, intercept, pcov, chisquare, dof))\n",
        "\n",
        "        #Plot\n",
        "        if(show_plots):\n",
        "            x_axis = np.linspace(x.min(), x.max(), 100)\n",
        "            ax[0,i].scatter(x, y, label=\"Data points\")\n",
        "            ax[0,i].plot(x_axis, (slope * x_axis) + intercept, color = 'darkred', label = 'Fit')\n",
        "            ax[0,i].set_ylabel(f\"$\\log(P_{m}({feature})$)\")\n",
        "            ax[0,i].set_xlabel(f\"$\\log({feature})$\")\n",
        "            ax[0,i].grid()\n",
        "\n",
        "            #Residual plot\n",
        "            ax[1,i].errorbar(x, residuals, yerr = e, fmt = 'o', capsize = 2.5, label=\"Residual\")\n",
        "            ax[1,i].hlines(0,x.min(), x.max(), color='darkred', label = 'Fit')\n",
        "            ax[1,i].grid()\n",
        "\n",
        "    if(show_plots):\n",
        "        ax[0,-1].legend()\n",
        "        ax[1,-1].legend()\n",
        "        fig.suptitle('Linear regressions and residuals' + spec, fontsize = 18)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    return fit_res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7PKBDxmZN6_"
      },
      "source": [
        "As features we need waiting times, that we can get by applying the `diff()` function to the 'Time' column of dataframe starting from the second row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "j17ZFVHEZN7A",
        "outputId": "acd089b4-9853-41aa-da36-8e523815412c"
      },
      "outputs": [],
      "source": [
        "#Inizializing lists\n",
        "magnitudes = [2, 3, 4, 5]\n",
        "bins = []\n",
        "counts = []\n",
        "errors = []\n",
        "\n",
        "for i, m in enumerate(magnitudes):\n",
        "\n",
        "    #Array of waiting times\n",
        "    wt = np.array(df[df['Magnitude'] >= m][['Time']].sort_values('Time').diff())[1:]\n",
        "\n",
        "    #Prepare data for plot\n",
        "    b, c, e = log_plot(wt)\n",
        "    bins.append(b)\n",
        "    counts.append(c)\n",
        "    errors.append(e)\n",
        "\n",
        "    #Plot\n",
        "    plt.scatter(10 ** b, 10 ** c, alpha = 0.75, label = f\"m $\\geq$ {m}\")\n",
        "    plt.xscale('log', base = 10)\n",
        "    plt.yscale('log', base = 10)\n",
        "\n",
        "plt.xlabel('Waiting times ($s$)')\n",
        "plt.ylabel('Probability density ($Hz$)')\n",
        "plt.title('Distribution of waiting times for different magnitudes')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We expect a power-law distribution of the waiting times for all magnitudes of type\n",
        "\n",
        "$$P_m(wt)=kwt^A$$\n",
        "\n",
        "Performing a logarithmic transformation on the data that gives\n",
        "\n",
        "$$\\log_{10}P_m(wt)=A\\log_{10}(wt)+B$$\n",
        "\n",
        "i.e. a linear distribution.\n",
        "\n",
        "So, in order to prove this, we will apply a linear regression and verify this is the law followed by data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB8f7S0OZN7A"
      },
      "source": [
        "Now we can apply the `cut_tails()` function in order to select only the linear regime of the distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSjkfg5AZN7I"
      },
      "outputs": [],
      "source": [
        "pearson_list = []\n",
        "last_cut_list = []\n",
        "\n",
        "counts_cut = counts[:]\n",
        "errors_cut = errors[:]\n",
        "bins_cut = bins[:]\n",
        "\n",
        "for i, m in enumerate(magnitudes):\n",
        "    \n",
        "    bins_cut[i], counts_cut[i], errors_cut[i], pearson = cut_tails(bins[i], counts[i], errors[i], threshold = 2)\n",
        "    pearson_list.append(pearson[-1])\n",
        "    last_cut_list.append(bins_cut[i][-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqpUNyfGZN7J"
      },
      "source": [
        "Finally, we perform the linear regression and visualize both the fit and residuals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "q49Dz3-5ZN7J",
        "outputId": "f1328b95-f9d4-4a87-8d6e-865231810a5f"
      },
      "outputs": [],
      "source": [
        "fit_res = statistic_plots(bins_cut, counts_cut, magnitudes, errors_cut, 'wt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPZl6wDOZN7K"
      },
      "source": [
        "We display the results of the linear fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtQLVycnZN7K",
        "outputId": "a99e9ced-8314-44e4-fa00-ff70adf4a636"
      },
      "outputs": [],
      "source": [
        "slopes = []\n",
        "intercepts = []\n",
        "sigmas_slope = []\n",
        "sigmas_intercept = []\n",
        "\n",
        "for res, m, p in zip(fit_res, magnitudes, pearson_list):\n",
        "    slope, intercept, pcov, chisquare , dof = res\n",
        "    confidence_level = (1.- chi2.cdf(chisquare, dof))*100\n",
        "    \n",
        "    slopes.append(slope)\n",
        "    intercepts.append(intercept)\n",
        "\n",
        "    sigma_slope, sigma_intercept = np.sqrt(np.diag(pcov))\n",
        "    \n",
        "    sigmas_slope.append(sigma_slope)\n",
        "    sigmas_intercept.append(sigma_intercept)\n",
        "\n",
        "    print(f\"------ Earthquakes with magnitude m ≥ {m}  ------\")\n",
        "    print(f\"Slope: {slope:.3f} ± {sigma_slope:.3f}\")\n",
        "    print(f\"Intercept: {intercept:.3f} ± {sigma_intercept:.3f}\")\n",
        "    print(f\"Pearson coefficient: {p:.2f}\")\n",
        "    print(f\"Degrees of freedom: {dof}\")\n",
        "    #print(f\"Chi2: {chisquare:.2f}\")\n",
        "    #print(f\"Hypotesis accepted with confidence level {confidence_level:.1f}%\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "wU_x63FoZN7L",
        "outputId": "ad2bf2f1-86fb-4403-dbcc-c555849fa5d4"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 3, figsize = (16,6))\n",
        "\n",
        "ax[0].scatter(magnitudes, last_cut_list)\n",
        "ax[0].set_xlabel('Magnitudes')\n",
        "ax[0].set_ylabel('$wt_{max}$')\n",
        "ax[0].grid()\n",
        "\n",
        "ax[1].errorbar(magnitudes, slopes, sigmas_slope)\n",
        "ax[1].set_xlabel('Magnitudes')\n",
        "ax[1].set_ylabel('Slopes')\n",
        "ax[1].grid()\n",
        "\n",
        "ax[2].errorbar(magnitudes, intercepts, sigmas_intercept)\n",
        "ax[2].set_xlabel('Magnitudes')\n",
        "ax[2].set_ylabel('Intercepts')\n",
        "ax[2].grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We notice two relevant things:\n",
        "\n",
        "* Slopes and intercepts of linear regressions seems to be close to the same value: $-1$ for the former and $-1.5$ for the latter; we will later investigate whether these parameters are universal constants or they depends on the choice of magnitudes and maximum distances.\n",
        "\n",
        "* Let's define $wt_{max}$ as the maximum value of waiting time in linear regime. We can observe that, after this threshold the distribution drops significantly. Also, $wt_{max}$ increases with the growth of magnitude."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to collapse each distribution to a single trending line we produce a new plot of the probability density function normalized with respect to the regressions we obtained before versus the waiting times normalized by $wt_{max}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for b, c, e, m, last_cut, slope, intercept in zip(bins, counts, errors, magnitudes, last_cut_list, slopes, intercepts):\n",
        "    \n",
        "    c = c[b >= last_cut-2.5]\n",
        "    e = e[b >= last_cut-2.5]\n",
        "    b = b[b >= last_cut-2.5] \n",
        "        \n",
        "    plt.plot((10 ** b) / (10 ** last_cut), (10 ** c) / (10 ** (slope * b + intercept)), alpha = 1, linestyle = '--', marker = 'o', label = f\"m $\\geq$ {m}\")\n",
        "    plt.xscale('log', base = 10)\n",
        "    plt.yscale('log', base = 10)\n",
        "    \n",
        "plt.xlabel(\"$\\\\frac{wt}{wt_{max}}$\")\n",
        "plt.ylabel(\"$\\\\frac{P_m(wt)}{wt^A\\cdot10^B}$\")\n",
        "plt.title(\"Scaling picture plot\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpcQMs6CZN7M"
      },
      "source": [
        "# 4. Distribution $P_m(r)$ of the Distances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tQm5lsxZN7N"
      },
      "source": [
        "We repeat what we did in the previous point for the distances between two consecutive measurement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "5gWGzgU7ZN7N",
        "outputId": "88e44226-aafb-47e3-cd25-810fec5fbaf9"
      },
      "outputs": [],
      "source": [
        "magnitudes = [2.5, 3, 3.5, 4, 4.5]\n",
        "\n",
        "bins   = []\n",
        "counts = []\n",
        "errors = []\n",
        "\n",
        "for i, m in enumerate(magnitudes):\n",
        "      #Array of siatcnes\n",
        "      df2 = df[df['Magnitude'] >= m].sort_values('Time')[['x', 'y', 'z']]\n",
        "      coordinates = df2.to_numpy()\n",
        "      d = np.linalg.norm(np.diff(coordinates, axis = 0), axis = 1)\n",
        "\n",
        "      #Prepare data for plot\n",
        "      b, c, e = log_plot(d)\n",
        "      bins.append(b)\n",
        "      counts.append(c)\n",
        "      errors.append(e)\n",
        "\n",
        "      #Plot\n",
        "      plt.scatter(10 ** b, 10 ** c, alpha = 0.75, label = f\"m $\\geq$ {m}\")\n",
        "      plt.xscale('log', base = 10)\n",
        "      plt.yscale('log', base = 10)\n",
        "\n",
        "plt.xlabel('Distances ($m$)')\n",
        "plt.ylabel('Probability density ($m^{-1}$)')\n",
        "plt.title('Distribution of distances for different magnitudes')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP2Ij_OdZN7N"
      },
      "outputs": [],
      "source": [
        "pearson_list = []\n",
        "last_cut_list = []\n",
        "\n",
        "counts_cut = counts[:]\n",
        "errors_cut = errors[:]\n",
        "bins_cut = bins[:]\n",
        "\n",
        "for i, m in enumerate(magnitudes):\n",
        "    \n",
        "    bins_cut[i], counts_cut[i], errors_cut[i], pearson = cut_tails(bins[i], counts[i], errors[i], threshold = 2)\n",
        "    pearson_list.append(pearson[-1])\n",
        "    last_cut_list.append(bins_cut[i][-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "2QafS392ZN7N",
        "outputId": "5429dd99-b6c1-4cc8-a7df-3ba6ecaa26bb"
      },
      "outputs": [],
      "source": [
        "fit_res = statistic_plots(bins_cut, counts_cut, magnitudes, errors_cut, 'd')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZCCRkJwZN7O"
      },
      "source": [
        "We display the results of the linear fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHS-N6oOZN7O",
        "outputId": "d0d6fd6a-dc78-45ad-c87a-459f429df055"
      },
      "outputs": [],
      "source": [
        "slopes = []\n",
        "intercepts = []\n",
        "sigmas_slope = []\n",
        "sigmas_intercept = []\n",
        "\n",
        "for res, m, p in zip(fit_res, magnitudes, pearson_list):\n",
        "    slope, intercept, pcov, chisquare , dof = res\n",
        "    confidence_level = (1.- chi2.cdf(chisquare, dof))*100\n",
        "    \n",
        "    slopes.append(slope)\n",
        "    intercepts.append(intercept)\n",
        "\n",
        "    sigma_slope, sigma_intercept = np.sqrt(np.diag(pcov))\n",
        "    \n",
        "    sigmas_slope.append(sigma_slope)\n",
        "    sigmas_intercept.append(sigma_intercept)\n",
        "\n",
        "    print(f\"------ Earthquakes with magnitude m ≥ {m}  ------\")\n",
        "    print(f\"Slope: {slope:.3f} ± {sigma_slope:.3f}\")\n",
        "    print(f\"Intercept: {intercept:.3f} ± {sigma_intercept:.3f}\")\n",
        "    print(f\"Pearson coefficient: {p:.2f}\")\n",
        "    print(f\"Degrees of freedom: {dof}\")\n",
        "    #print(f\"Chi2: {chisquare:.2f}\")\n",
        "    #print(f\"Hypotesis accepted with confidence level {confidence_level:.1f}%\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 3, figsize = (16,6))\n",
        "\n",
        "ax[0].scatter(magnitudes, last_cut_list)\n",
        "ax[0].set_xlabel('Magnitudes')\n",
        "ax[0].set_ylabel('$wt_{max}$')\n",
        "ax[0].grid()\n",
        "\n",
        "ax[1].errorbar(magnitudes, slopes, sigmas_slope)\n",
        "ax[1].set_xlabel('Magnitudes')\n",
        "ax[1].set_ylabel('Slopes')\n",
        "ax[1].grid()\n",
        "\n",
        "ax[2].errorbar(magnitudes, intercepts, sigmas_intercept)\n",
        "ax[2].set_xlabel('Magnitudes')\n",
        "ax[2].set_ylabel('Intercepts')\n",
        "ax[2].grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In contrast to waiting times, it seems there isn't a clear correlation between magnitudes and $d_{max}$, so we will recreate the scaling picture plot without normalizing the x axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for b, c, e, m, last_cut, slope, intercept in zip(bins, counts, errors, magnitudes, last_cut_list, slopes, intercepts):\n",
        "    \n",
        "    c = c[b >= last_cut-2.5]\n",
        "    e = e[b >= last_cut-2.5]\n",
        "    b = b[b >= last_cut-2.5] \n",
        "        \n",
        "    plt.plot((10 ** b), (10 ** c) / (10 ** (slope * b + intercept)), alpha = 1, linestyle = '--', marker = 'o', label = f\"m $\\geq$ {m}\")\n",
        "    plt.xscale('log', base = 10)\n",
        "    plt.yscale('log', base = 10)\n",
        "    \n",
        "plt.xlabel(\"$d \\ (m)$\")\n",
        "plt.ylabel(\"$\\\\frac{P_m(d)}{d^A\\cdot10^B}$\")\n",
        "plt.title(\"Scaling picture plot\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWvwKsnsZN7P"
      },
      "source": [
        "# 5. Distribution $P_{m,R}(t)$ of Waiting Times for events at Distance $r<R$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWqJOHGEZN7P"
      },
      "source": [
        "We construct a function  - called `get_Pmr()` - that, for a given value of magnitude $m$ and relative distance $R$, selects the events having magnitude $\\geqslant m$ and relative distances $\\leqslant R$; finally, it computes the waiting times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HYVDdbeZN7Q"
      },
      "outputs": [],
      "source": [
        "def get_Pmr(df, m, R):\n",
        "\n",
        "    dfm = df[df['Magnitude']>= m].sort_values('Time')[['Time', 'x', 'y', 'z']]\n",
        "\n",
        "    #Evaluate the distance between one event and the next one\n",
        "    coordinates = dfm.to_numpy()\n",
        "    d = np.linalg.norm(np.diff(coordinates, axis = 0), axis = 1)\n",
        "    d = np.insert(d, 0, 0)\n",
        "\n",
        "    dfm['Rel_distance'] = d\n",
        "    dfmr = dfm[dfm['Rel_distance'] <= R].sort_values('Time') #Select by distance\n",
        "\n",
        "    wt = np.array(dfmr['Time'].diff())[1:]\n",
        "\n",
        "    return dfmr, d, wt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZb9Fe4xZN7Q"
      },
      "source": [
        "Now we can plot the results using two nested cycles: with the outer one we select the magnitude and the inner one the relative distance.\n",
        "\n",
        "In contrast to the previous calculations we excluded the highest magnitude $m=5$ because the plot resulted into being too unpopulated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YqrOXRHaZN7S",
        "outputId": "0963598c-1e73-445d-e5a3-5f5fca465397"
      },
      "outputs": [],
      "source": [
        "magnitudes = [2, 2.5, 3, 3.5, 4]\n",
        "Radii = [1e4, 5e4, 1e5, 5e5, 1e6]\n",
        "\n",
        "bins   = [[] for _ in range(len(Radii))]\n",
        "counts = [[] for _ in range(len(Radii))]\n",
        "errors = [[] for _ in range(len(Radii))]\n",
        "\n",
        "for j, radius in enumerate(Radii):\n",
        "    index = 0\n",
        "    fig, ax = plt.subplots(1, len(magnitudes), figsize = (14,5))\n",
        "\n",
        "    for i, m in enumerate(magnitudes):\n",
        "        dfmr, d, wt = get_Pmr(df, m, radius)\n",
        "\n",
        "        b, c, e = log_plot(wt)\n",
        "        bins[j].append(b)\n",
        "        counts[j].append(c)\n",
        "        errors[j].append(e)\n",
        "\n",
        "        ax[index].scatter(10 ** b, 10 ** c, edgecolor = 'black', marker = '.')\n",
        "        ax[index].set_title(f'm $\\geqslant$ {m} and R $\\leqslant$ {radius:.0f}')\n",
        "        ax[index].set_yscale('log', base = 10)\n",
        "        ax[index].set_xscale('log', base = 10)\n",
        "\n",
        "        ax[index].set_ylabel(f\"$\\log(P_{m}(wt)$)\")\n",
        "        ax[index].set_xlabel(f\"$\\log(wt)$\")\n",
        "        ax[index].grid()\n",
        "\n",
        "        index += 1\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pearson_list = []\n",
        "last_cut_list = [[] for _ in range(len(Radii))]\n",
        "\n",
        "counts_cut = counts[:]\n",
        "errors_cut = errors[:]\n",
        "bins_cut = bins[:]\n",
        "\n",
        "for i, m in enumerate(magnitudes):\n",
        "    for j, radius in enumerate(Radii):\n",
        "    \n",
        "        bins_cut[j][i], counts_cut[j][i], errors_cut[j][i], pearson = cut_tails(bins_cut[j][i], counts_cut[j][i], errors_cut[j][i], threshold = 2)\n",
        "        pearson_list.append(pearson[-1])\n",
        "        last_cut_list[j].append(bins_cut[j][i][-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "viGX5bdIZN7S",
        "outputId": "60c28599-b4d0-4fd1-8732-df17ad2906cd"
      },
      "outputs": [],
      "source": [
        "slopes = [[] for _ in range(len(Radii))]\n",
        "intercepts = [[] for _ in range(len(Radii))]\n",
        "sigmas_slope = [[] for _ in range(len(Radii))]\n",
        "sigmas_intercept = [[] for _ in range(len(Radii))]\n",
        "\n",
        "for j, radius in enumerate(Radii):\n",
        "\n",
        "    fit_res = statistic_plots(bins_cut[j], counts_cut[j], magnitudes, errors_cut[j], 'wt', spec = f' ($R\\leqslant{radius:.0f}$)')\n",
        "\n",
        "    for res, m, p in zip(fit_res, magnitudes, pearson_list):\n",
        "        slope, intercept, pcov, chisquare , dof = res\n",
        "        \n",
        "        slopes[j].append(slope)\n",
        "        intercepts[j].append(intercept)\n",
        "\n",
        "        confidence_level = (1.- chi2.cdf(chisquare, dof))*100\n",
        "\n",
        "        sigma_slope, sigma_intercept = np.sqrt(np.diag(pcov))\n",
        "        \n",
        "        sigmas_slope[j].append(sigma_slope)\n",
        "        sigmas_intercept[j].append(sigma_intercept)\n",
        "\n",
        "        print(f\"------ Earthquakes with m ≥ {m}, R ≤ {radius}  ------\")\n",
        "        print(f\"Slope: {slope:.3f} ± {sigma_slope:.3f}\")\n",
        "        print(f\"Intercept: {intercept:.3f} ± {sigma_intercept:.3f}\")\n",
        "        print(f\"Pearson coefficient: {p:.2f}\")\n",
        "        print(f\"Degrees of freedom: {dof}\")\n",
        "        #print(f\"Chi2: {chisquare:.2f}\")\n",
        "        #print(f\"Hypotesis accepted with confidence level {confidence_level:.1f}%\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, r in enumerate(Radii):\n",
        "    for b, c, e, m, last_cut, slope, intercept in zip(bins[i], counts[i], errors[i], magnitudes, last_cut_list[i], slopes[i], intercepts[i]):\n",
        "        \n",
        "        c = c[b >= last_cut-2.5]\n",
        "        e = e[b >= last_cut-2.5]\n",
        "        b = b[b >= last_cut-2.5] \n",
        "        \n",
        "        plt.plot((10 ** b) / (10 ** last_cut), (10 ** c) / (10 ** (slope * b + intercept)), alpha = 1, linestyle = '--', marker = 'o', label = f\"m $\\geq$ {m}\")\n",
        "        \n",
        "plt.xscale('log', base = 10)\n",
        "plt.yscale('log', base = 10)    \n",
        "plt.xlabel(\"$d \\ (m)$\")\n",
        "plt.ylabel(\"$\\\\frac{P_m(d)}{d^A\\cdot10^B}$\")\n",
        "plt.title(\"Scaling picture plot\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, r in enumerate(Radii):\n",
        "    for j, (b, c, e, m, last_cut, slope, intercept) in enumerate(zip(bins[i], counts[i], errors[i], magnitudes[:-1], last_cut_list[i], slopes[i], intercepts[i])):\n",
        "        if i % 2 == 1 or j % 2 == 1:\n",
        "            continue\n",
        "        \n",
        "        c = c[b >= last_cut-2.5]\n",
        "        e = e[b >= last_cut-2.5]\n",
        "        b = b[b >= last_cut-2.5] \n",
        "        \n",
        "        plt.plot((10 ** b) / (10 ** last_cut), (10 ** c) / (10 ** (slope * b + intercept)), alpha = 1, linestyle = '--', marker = 'o', label = f\"m $\\geq$ {m}, R $\\leqslant {r}$\")\n",
        "        \n",
        "plt.xscale('log', base = 10)\n",
        "plt.yscale('log', base = 10)    \n",
        "plt.xlabel(\"$d \\ (m)$\")\n",
        "plt.ylabel(\"$\\\\frac{P_m(d)}{d^A\\cdot10^B}$\")\n",
        "plt.title(\"Scaling picture plot\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dependence of the fit parameters to m and R"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Previously we verified that each distribution obey a power-law regime in a defined domain of waiting times; now, we ask ourselves if these power-laws are defined by universal parameters or if those parameters depends on $m$ and $R$. To perform this investigation, we display below two heatmaps showing the values of $A$ and $B$ for different $m$ and $R$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "magnitudes = np.linspace(2, 4.1, 24)  \n",
        "Radii = np.linspace(1e4, 1e6, 24)     \n",
        "\n",
        "bins   = [[] for _ in range(len(Radii))]\n",
        "counts = [[] for _ in range(len(Radii))]\n",
        "errors = [[] for _ in range(len(Radii))]\n",
        "\n",
        "for j, radius in enumerate(Radii):\n",
        "    for i, m in enumerate(magnitudes):\n",
        "        dfmr, dd, wt = get_Pmr(df, m, radius)\n",
        "        \n",
        "        b, c, e = log_plot(wt)\n",
        "        bins[j].append(b)\n",
        "        counts[j].append(c)\n",
        "        errors[j].append(e)\n",
        "\n",
        "pearson_list = []\n",
        "\n",
        "for i, m in enumerate(magnitudes):\n",
        "    for j, radius in enumerate(Radii):\n",
        "        bins[j][i], counts[j][i], errors[j][i], pearson = cut_tails(bins[j][i], counts[j][i], errors[j][i], threshold = 2)\n",
        "        pearson_list.append(pearson[-1])\n",
        "\n",
        "fits_data = []\n",
        "\n",
        "for j, radius in enumerate(Radii):\n",
        "\n",
        "    fit_res = statistic_plots(bins[j], counts[j], magnitudes, errors[j], 'wt', spec = f' ($R\\leqslant{radius:.0f}$)', show_plots=False)\n",
        "    fits_data.append([])\n",
        "    \n",
        "    for res, m, p in zip(fit_res, magnitudes, pearson_list):\n",
        "        slope, intercept, pcov, chisquare , dof = res\n",
        "        \n",
        "        confidence_level = (1.- chi2.cdf(chisquare, dof))*100\n",
        "        \n",
        "        sigma_slope, sigma_intercept = np.sqrt(np.diag(pcov))\n",
        "        fits_data[-1].append([slope, intercept, sigma_slope, sigma_intercept])\n",
        "        '''        \n",
        "        print(f\"------ Earthquakes with m ≥ {m}, R ≤ {radius}  ------\")\n",
        "        print(f\"Slope: {slope:.3f} ± {sigma_slope:.3f}\")\n",
        "        print(f\"Intercept: {intercept:.3f} ± {sigma_intercept:.3f}\")\n",
        "        print(f\"Pearson coefficient: {p:.2f}\")\n",
        "        print(f\"Degrees of freedom: {dof}\")\n",
        "        #print(f\"Chi2: {chisquare:.2f}\")\n",
        "        #print(f\"Hypotesis accepted with confidence level {confidence_level:.1f}%\")\n",
        "        print()'''\n",
        "fits_data = np.array(fits_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_to_plot = fits_data[:,:,0]\n",
        "\n",
        "fig = plt.figure(figsize=(12,5))\n",
        "\n",
        "ax1 = fig.add_subplot(1,2,1)\n",
        "plot1 = ax1.contourf(data_to_plot)\n",
        "ax1.set_xlabel(\"Magnitudes\")\n",
        "ax1.set_ylabel(\"Maximum R [m]\")\n",
        "ax1.set_title(\"Slope Contour Plot\")\n",
        "fig.colorbar(plot1, ax=ax1)  \n",
        "\n",
        "x_tick_step = 4\n",
        "y_tick_step = 4\n",
        "ax1.set_xticks(np.arange(0, len(magnitudes), x_tick_step), [\"{:.1f}\".format(mag) for mag in magnitudes[::x_tick_step]])\n",
        "ax1.set_yticks(np.arange(0, len(Radii), y_tick_step), [\"{:.0e}\".format(r) for r in Radii[::y_tick_step]])\n",
        "\n",
        "\n",
        "x, y = np.meshgrid(magnitudes, Radii)\n",
        "ax2 = fig.add_subplot(1,2,2, projection='3d')\n",
        "scat2 = ax2.plot_surface(x, y, data_to_plot, cmap='viridis', linewidth=0, antialiased=False)\n",
        "ax2.set_xlabel(\"Magnitudes\")\n",
        "ax2.set_ylabel(\"Maximum R [m]\")\n",
        "ax2.set_title(\"Slope 3D Plot\")\n",
        "fig.colorbar(scat2, ax=ax2, shrink=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_to_plot = fits_data[:,:,1]\n",
        "\n",
        "fig = plt.figure(figsize=(12,5))\n",
        "\n",
        "ax1 = fig.add_subplot(1,2,1)\n",
        "plot1 = ax1.contourf(data_to_plot)\n",
        "ax1.set_xlabel(\"Magnitudes\")\n",
        "ax1.set_ylabel(\"Maximum R [m]\")\n",
        "ax1.set_title(\"Slope Contour Plot\")\n",
        "fig.colorbar(plot1, ax=ax1)  \n",
        "\n",
        "x_tick_step = 4\n",
        "y_tick_step = 4\n",
        "ax1.set_xticks(np.arange(0, len(magnitudes), x_tick_step), [\"{:.1f}\".format(mag) for mag in magnitudes[::x_tick_step]])\n",
        "ax1.set_yticks(np.arange(0, len(Radii), y_tick_step), [\"{:.0e}\".format(r) for r in Radii[::y_tick_step]])\n",
        "\n",
        "\n",
        "x, y = np.meshgrid(magnitudes, Radii)\n",
        "ax2 = fig.add_subplot(1,2,2, projection='3d')\n",
        "scat2 = ax2.plot_surface(x, y, data_to_plot, cmap='viridis', linewidth=0, antialiased=False)\n",
        "ax2.set_xlabel(\"Magnitudes\")\n",
        "ax2.set_ylabel(\"Maximum R [m]\")\n",
        "ax2.set_title(\"Slope 3D Plot\")\n",
        "ax2.view_init(30,60)\n",
        "fig.colorbar(scat2, ax=ax2, shrink=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From these plots we can see a dependance of the parameters with respect to $m$ and $R$. So, the power-laws are not universal, but one can try to extrapolate an empirical law through a curve fit on these maps (probably a quadratic one is a good approximation given the shape of the heatmaps)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nvRlwC6k3jR"
      },
      "source": [
        "# 6. Scaling Picture Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzVWIVdBZN7U"
      },
      "source": [
        "## 6.1 Tree structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "bFArPaZsZN7V",
        "outputId": "67ad3a44-5d91-4b9b-fb26-1a706ec84a01"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "df['Fol event'] = [[] for _ in range(len(df))]\n",
        "\n",
        "for i in df['Index']:\n",
        "    if df['Prev_event'][i] > -1:\n",
        "        df['Fol event'][df['Prev_event'][i]].append(i)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNXL7zxoZN7V",
        "outputId": "8d40d213-7d09-436f-f271-13c29da77195"
      },
      "outputs": [],
      "source": [
        "#Let's select only the events with the biggest number of sons\n",
        "#Let's consider only starting from 10\n",
        "\n",
        "#voglio selezionare soltanto gli elementi che hanno un tot numero di terremoti \"figli\"sr\n",
        "#richiediamo che len(df['Fol event']) > 10, ossia il terremoto in questione ha generato almeno 10 figli\n",
        "\n",
        "indeces = []\n",
        "sons = []\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "    if len(df['Fol event'][i]) > 100:\n",
        "        indeces.append(df['Index'][i])\n",
        "        sons.append(df['Fol event'][i])\n",
        "\n",
        "#selezioniamo gli indici in base ai terremoti che hanno avuto il maggior numero di eventi (+ di 10)\n",
        "\n",
        "print('Number of eartquakes selected: ', len(indeces)) #stampo qui il numero di terremoti che ho selezionato con il constraint che ho imposto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "idkoRU1FZN7V",
        "outputId": "72085ef8-94bc-44f8-8736-af43fdc2f410"
      },
      "outputs": [],
      "source": [
        "#Let's perform a plot to visualize the distribution of points in space\n",
        "#It can be removed\n",
        "\n",
        "list_ = [len(sons[i]) for i in range(len(indeces))]\n",
        "\n",
        "store = []\n",
        "\n",
        "for i in indeces:\n",
        "    rand = np.random.uniform()\n",
        "\n",
        "    color = np.random.rand(3)\n",
        "    index = [i for _ in range(len(df['Fol event'][i]))]\n",
        "    store.append(index)\n",
        "\n",
        "    plt.scatter(index, df['Fol event'][i], c = [color]*len(index), cmap = 'viridis', s = 1)\n",
        "\n",
        "\n",
        "plt.xlabel('Father (Index)')\n",
        "plt.ylabel('Son (Index)')\n",
        "plt.title('Earthquakes distribution as a function of the original earthquake')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3ppSLCDZN7V"
      },
      "source": [
        "**Construction of the hierarchy linkage:**\n",
        "\n",
        "Let's perform the hierarchical/agglomerative clustering. The input y may be either a 1-D condensed distance matrix or a 2-D array of observation vectors. The linkage function is used to compute the distance matrix for the points in space. Each point will be represented by the index of the father and the index of the son, in the following way:\n",
        "\n",
        "[[0,1], [0,2], [0,3], ..., [0, 10] , ... ]\n",
        "\n",
        "where 0 is the original earthquake that actually causes the origin of earthquake 10. Linkage function will divide the points in clusters using as metric the centroid method: distance between different clusters will be evaluated computing the distance between the centroids. This is used to guarantee the indipendency of the events.\n",
        "\n",
        "Let's apply the **dendrogram algorithm**: The dendrogram illustrates how each cluster is composed by drawing a U-shaped link between a non-singleton cluster and its children. The top of the U-link indicates a cluster merge. The two legs of the U-link indicate which clusters were merged. The length of the two legs of the U-link represents the distance between the child clusters. It is also the cophenetic distance between original observations in the two children clusters.\n",
        "\n",
        "Let's see the method used:\n",
        "\n",
        "* truncate_mode: shows only the first p clusters, while the remaining will be represented as leaves\n",
        "* count_sort\n",
        "\n",
        "*False*: nothing is done\n",
        "\n",
        "*ascending*: the child with the minimum number of original objects in its cluster is plotted first\n",
        "\n",
        "*descending*: the child with the maximum number of original objects in its cluster is plotted first\n",
        "\n",
        "* show_contracted: when True the heights of non-singleton nodes contracted into a leaf node are plotted as crosses along the link connecting that leaf node. This really is only useful when truncation is used.\n",
        "\n",
        "The output of the function is a dendrogram: on the x axis there are the labels, they can be accessed using d['ivl']. They represent the index of the earthquake that is been generated. Please notice that in most cases the label will be something like (20), because we are using the truncated mode and a single node is representing 20 earthquakes (too many data). On the y axis there will be the distance between clusters: the higher is the value of y, the biggest is the distance between different clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8lMugUfPZN7W",
        "outputId": "e7956d60-0d8e-4fb1-e2df-af78ad732e77"
      },
      "outputs": [],
      "source": [
        "L = [[store[i][j], sons[i][j]] for i in range(len(store)) for j in range(len(store[i]))]\n",
        "#L is a list containing father and sons\n",
        "\n",
        "a = list_[0]\n",
        "b = list_[1]\n",
        "\n",
        "s = 2\n",
        "i = 0\n",
        "counter = 1\n",
        "\n",
        "index = 0\n",
        "\n",
        "while s<len(list_):\n",
        "    if counter == 1 and s == 2:\n",
        "        fig, axes = plt.subplots(1, 3, figsize = (20,5))\n",
        "\n",
        "    a = b+1\n",
        "    b += list_[s]\n",
        "\n",
        "    S = np.array(L[a : b]) #divido in base ai padri\n",
        "    labels = [S[j][1] for j in range(len(S))]\n",
        "\n",
        "    z = linkage(S, method = 'centroid', optimal_ordering = False )\n",
        "    #optimal_ordering = True migliora il plot finale ma il run si allunga, runnare prima della consegna\n",
        "\n",
        "\n",
        "    d = dendrogram(z, truncate_mode = 'lastp', p = 15, count_sort = 'descending', show_contracted = True, leaf_rotation = 50, labels = labels, ax = axes[index] )\n",
        "    #inserendo p = 5 vedo soltanto 5 labels, le restanti sono raggruppate in centroidi\n",
        "    #con p posso variare il numero di labels visibili, ogni label sarà ravvicinata ad un dato terremoto\n",
        "    #labels: voglio che usi come label per l'algoritmo gli index dei figli\n",
        "\n",
        "\n",
        "\n",
        "    axes[index].set_title(f'Dendogram for earthquake {indeces[i]}')\n",
        "\n",
        "    s += 1\n",
        "    i += 1\n",
        "    counter += 1\n",
        "    index+= 1\n",
        "\n",
        "    if i % 3 == 0 and s < len(list_) - 1:\n",
        "        fig, axes = plt.subplots(1, 3, figsize = (20,5))\n",
        "        counter = 1\n",
        "        index = 0\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.1.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
